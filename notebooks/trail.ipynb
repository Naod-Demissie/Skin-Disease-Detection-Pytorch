{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Optional, Tuple, List\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "ROOT_DIR = 'D:/Projects/Skin-Disease-Detection-Pytorch'\n",
    "sys.path.append(ROOT_DIR)\n",
    "from src.utils.config import *\n",
    "from src.utils.dataloader import *\n",
    "from src.utils.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82432d829994c6ebf35e8eff363ca88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rdadmin/Documents/Naod-Projects/Skin-Disease-Detection-Pytorch/notebooks/trail.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2248504331227d/home/rdadmin/Documents/Naod-Projects/Skin-Disease-Detection-Pytorch/notebooks/trail.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m _model_checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(save_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mCKPT_DIR\u001b[39m}\u001b[39;00m\u001b[39m/pretraining/\u001b[39m\u001b[39m{\u001b[39;00mtrail_name\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2248504331227d/home/rdadmin/Documents/Naod-Projects/Skin-Disease-Detection-Pytorch/notebooks/trail.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2248504331227d/home/rdadmin/Documents/Naod-Projects/Skin-Disease-Detection-Pytorch/notebooks/trail.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     model\u001b[39m=\u001b[39mMODEL,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2248504331227d/home/rdadmin/Documents/Naod-Projects/Skin-Disease-Detection-Pytorch/notebooks/trail.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     device\u001b[39m=\u001b[39mDEVICES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2248504331227d/home/rdadmin/Documents/Naod-Projects/Skin-Disease-Detection-Pytorch/notebooks/trail.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     scheduler\u001b[39m=\u001b[39m _scheduler\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2248504331227d/home/rdadmin/Documents/Naod-Projects/Skin-Disease-Detection-Pytorch/notebooks/trail.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2248504331227d/home/rdadmin/Documents/Naod-Projects/Skin-Disease-Detection-Pytorch/notebooks/trail.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_dataloader, val_dataloader)\n",
      "File \u001b[0;32m~/Documents/Naod-Projects/Skin-Disease-Detection-Pytorch/notebooks/../src/utils/trainer.py:148\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_epoch):\n\u001b[0;32m--> 148\u001b[0m     train_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_step(dataloader\u001b[39m=\u001b[39;49mtrain_loader, epoch\u001b[39m=\u001b[39;49mepoch)\n\u001b[1;32m    149\u001b[0m     val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_step(dataloader\u001b[39m=\u001b[39mval_loader,epoch\u001b[39m=\u001b[39mepoch)\n\u001b[1;32m    151\u001b[0m     checkpoint_monitor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_checkpoint\u001b[39m.\u001b[39mmonitor\n",
      "File \u001b[0;32m~/Documents/Naod-Projects/Skin-Disease-Detection-Pytorch/notebooks/../src/utils/trainer.py:74\u001b[0m, in \u001b[0;36mTrainer._train_step\u001b[0;34m(self, dataloader, epoch)\u001b[0m\n\u001b[1;32m     71\u001b[0m metric_name \u001b[39m=\u001b[39m metric_fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m.\u001b[39mrstrip(\u001b[39m'\u001b[39m\u001b[39m_score\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m metric_fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy_score\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     73\u001b[0m     metric_value \u001b[39m=\u001b[39m metric_fn(\n\u001b[0;32m---> 74\u001b[0m         targets\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy(),\n\u001b[1;32m     75\u001b[0m         predicted\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     metric_value \u001b[39m=\u001b[39m metric_fn(\n\u001b[1;32m     79\u001b[0m         targets\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(),\n\u001b[1;32m     80\u001b[0m         predicted\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(),\n\u001b[1;32m     81\u001b[0m         average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     82\u001b[0m         zero_division\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     83\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, recall_score, \n",
    "                             precision_score, f1_score)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils.config import *\n",
    "from src.utils.dataloader import dermnet_dataloader\n",
    "from src.utils.model import Network\n",
    "from src.utils.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from src.utils.trainer import Trainer\n",
    "\n",
    "\n",
    "base_model_names = [\n",
    "    'resnet18',\n",
    "    'resnet50',\n",
    "    'efficientnet_b0', \n",
    "    'efficientnet_v2_s', \n",
    "]\n",
    "\n",
    "for base_model_name in base_model_names:\n",
    "    trail_name = base_model_name\n",
    "    train_dataloader, val_dataloader, test_dataloader = dermnet_dataloader(\n",
    "        resize_size=RESIZE_SIZE[base_model_name],\n",
    "        crop_size=CROP_SIZE[base_model_name],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    MODEL = Network(\n",
    "        base_model=get_base_model(base_model_name),\n",
    "        dropout=None,\n",
    "        output_dims=[],\n",
    "        num_classes=len(DERMNET_LABEL_NAME)\n",
    "    )\n",
    "\n",
    "    _optimizer = Adam(MODEL.parameters(), lr=LEARNING_RATE)\n",
    "    _scheduler = ReduceLROnPlateau(_optimizer, mode='min', patience=5, factor=0.1, verbose=True) #TODO set the monitor\n",
    "    _early_stop = EarlyStopping(patience=30, monitor='val_accuracy', mode='max')\n",
    "    _tensorboard = TensorBoard(log_dir=f'{TB_LOG_DIR}/pretraining/{trail_name}')\n",
    "    _model_checkpoint = ModelCheckpoint(save_path=f'{CKPT_DIR}/pretraining/{trail_name}.pt', monitor='val_accuracy', mode='max')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=MODEL,\n",
    "        device=DEVICES,\n",
    "        num_epoch=NUM_EPOCHS,\n",
    "        metrics=[accuracy_score, recall_score, precision_score, f1_score],\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(),\n",
    "        early_stop = _early_stop,\n",
    "        model_checkpoint = _model_checkpoint,\n",
    "        tensorboard = _tensorboard,\n",
    "        optimizer = _optimizer,\n",
    "        scheduler= _scheduler\n",
    "    )\n",
    "    \n",
    "    trainer.fit(train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DermnetDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data: Tuple[str, int], \n",
    "            transform: Optional[transforms.Compose] = None\n",
    "    ) -> None:\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image_path, label = self.data[index]\n",
    "        image = Image.open(image_path)\n",
    "        label = torch.tensor(label)\n",
    "        label = F.one_hot(label, num_classes=len(DERMNET_LABEL_NAME)).float()\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, image_path\n",
    "\n",
    "\n",
    "\n",
    "def _transforms(resize_size, crop_size):\n",
    "    train_transform = transforms.Compose(\n",
    "            [\n",
    "                # transforms.ToPILImage(),\n",
    "                transforms.Resize(resize_size[:-1]),\n",
    "                transforms.CenterCrop(crop_size[:-1]),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "    val_transform = transforms.Compose(\n",
    "            [\n",
    "                # transforms.ToPILImage(),\n",
    "                transforms.Resize(resize_size[:-1]),\n",
    "                transforms.CenterCrop(crop_size[:-1]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    test_transform= transforms.Compose(\n",
    "            [\n",
    "                # transforms.ToPILImage(),\n",
    "                transforms.Resize(resize_size[:-1]),\n",
    "                transforms.CenterCrop(crop_size[:-1]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    return train_transform, val_transform, test_transform\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def prepare_dermnet_data(\n",
    "        resize_size: Tuple[int, int, int], \n",
    "        crop_size: Tuple[int, int, int], \n",
    "        batch_size: int, \n",
    "        num_workers: int\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \n",
    "    DERMNET_DATA_DIR = 'D:/Datasets/Skin-Disease-Detection/dataset/Dermnet'\n",
    "    label_names = os.listdir(f'{DERMNET_DATA_DIR}/train')\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    for label in label_names:\n",
    "        file_paths = glob(f'{DERMNET_DATA_DIR}/train/{label}/*')\n",
    "        train_paths, val_paths = train_test_split(file_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "        sparse_label = label_names.index(label)\n",
    "        train_data += [(path, sparse_label) for path in train_paths]\n",
    "        val_data += [(path, sparse_label) for path in val_paths]\n",
    "\n",
    "    test_data = []\n",
    "    for label in label_names:\n",
    "        file_paths = glob(f'{DERMNET_DATA_DIR}/test/{label}/*')\n",
    "        sparse_label = label_names.index(label)\n",
    "        test_data += [(path, sparse_label) for path in file_paths]\n",
    "\n",
    "\n",
    "    train_transform, val_transform, test_transform = _transforms(resize_size[:-1], crop_size[:-1])\n",
    "\n",
    "    train_ds = DermnetDataset(data=train_data, transform=train_transform)\n",
    "    val_ds = DermnetDataset(data=val_data, transform=val_transform)\n",
    "    test_ds = DermnetDataset(data=test_data, transform=test_transform)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "torch.Size([3, 224, 224]) tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.]) D:/Datasets/Skin-Disease-Detection/dataset/Dermnet/train/Eczema Photos\\lichen-simplex-chronicus-114.jpg\n"
     ]
    }
   ],
   "source": [
    "DERMNET_DATA_DIR = 'D:/Datasets/Skin-Disease-Detection/dataset/Dermnet'\n",
    "label_names = os.listdir(f'{DERMNET_DATA_DIR}/train')\n",
    "train_data = []\n",
    "val_data = []\n",
    "for label in label_names:\n",
    "    file_paths = glob(f'{DERMNET_DATA_DIR}/train/{label}/*')\n",
    "    train_paths, val_paths = train_test_split(file_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "    sparse_label = label_names.index(label)\n",
    "    train_data += [(path, sparse_label) for path in train_paths]\n",
    "    val_data += [(path, sparse_label) for path in val_paths]\n",
    "\n",
    "# test_data = []\n",
    "# for label in label_names:\n",
    "#     file_paths = glob(f'{DERMNET_DATA_DIR}/test/{label}/*')\n",
    "#     sparse_label = label_names.index(label)\n",
    "#     test_data += [(path, sparse_label) for path in file_paths]\n",
    "\n",
    "\n",
    "train_transform, val_transform, test_transform = _transforms((256, 256), (224, 224))\n",
    "\n",
    "train_ds = DermnetDataset(data=train_data, transform=train_transform)\n",
    "# val_ds = DermnetDataset(data=val_data, transform=val_transform)\n",
    "# test_ds = DermnetDataset(data=test_data, transform=test_transform)\n",
    "\n",
    "img, label, image_path = train_ds.__getitem__(3200)\n",
    "print(img.shape, label, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Your continuous label tensor\n",
    "label = torch.from_numpy(np.arange(5, 10))\n",
    "\n",
    "# Convert to integer indices\n",
    "label_indices = (label.float() - label.min().float()).long()\n",
    "label_indices\n",
    "# Apply F.one_hot\n",
    "# one_hot_label = F.one_hot(label_indices, num_classes=7)\n",
    "\n",
    "# print(one_hot_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(base_model_name):\n",
    "    assert base_model_name in [\n",
    "        'resnet18',\n",
    "        'resnet50',\n",
    "        'efficientnet_b0', \n",
    "        'efficientnet_b1', \n",
    "        'efficientnet_b2', \n",
    "        'efficientnet_b3', \n",
    "        'efficientnet_b4', \n",
    "        'efficientnet_v2_s', \n",
    "        'efficientnet_v2_m'\n",
    "    ], f'Invalid base model name: {base_model_name}'\n",
    "\n",
    "    return (\n",
    "        resnet18(weights=None) if base_model_name == 'resnet18' else\n",
    "        resnet50(weights=None) if base_model_name == 'resnet50' else\n",
    "        efficientnet_b0(weights=None) if base_model_name == 'efficientnet_b0' else\n",
    "        efficientnet_b1(weights=None) if base_model_name == 'efficientnet_b1' else\n",
    "        efficientnet_b2(weights=None) if base_model_name == 'efficientnet_b2' else\n",
    "        efficientnet_b3(weights=None) if base_model_name == 'efficientnet_b3' else\n",
    "        efficientnet_b4(weights=None) if base_model_name == 'efficientnet_b4' else\n",
    "        efficientnet_v2_s(weights=None) if base_model_name == 'efficientnet_v2_s' else\n",
    "        efficientnet_v2_m(weights=None)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Network(nn.Module):\n",
    "#     def __init__(\n",
    "#             self, \n",
    "#             base_model, \n",
    "#             dropout: float, \n",
    "#             output_dims: List[int],\n",
    "#             num_classes: int\n",
    "#     ) -> None:\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.base_model = base_model\n",
    "#         input_dim: int = base_model.classifier[1].in_features\n",
    "\n",
    "#         layers: List[nn.Module] = []\n",
    "#         for output_dim in output_dims:\n",
    "#             layers.append(nn.Linear(input_dim, output_dim))\n",
    "#             layers.append(nn.ReLU())\n",
    "#             layers.append(nn.Dropout(dropout))\n",
    "#             input_dim = output_dim\n",
    "#         layers.append(nn.Linear(input_dim, num_classes))\n",
    "#         layers.append(nn.Softmax(dim=-1))\n",
    "\n",
    "#         self.base_model.classifier = nn.Sequential(*layers)\n",
    "    \n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         return self.base_model(x)\n",
    "    \n",
    "\n",
    "# class ResNetwork(nn.Module):\n",
    "#     def __init__(\n",
    "#             self, \n",
    "#             base_model, \n",
    "#             dropout: float, \n",
    "#             output_dims: List[int],\n",
    "#             num_classes: int\n",
    "#     ) -> None:\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.base_model = base_model\n",
    "#         input_dim: int = base_model.fc.in_features\n",
    "\n",
    "#         layers: List[nn.Module] = []\n",
    "#         for output_dim in output_dims:\n",
    "#             layers.append(nn.Linear(input_dim, output_dim))\n",
    "#             layers.append(nn.ReLU())\n",
    "#             layers.append(nn.Dropout(dropout))\n",
    "#             input_dim = output_dim\n",
    "#         layers.append(nn.Linear(input_dim, num_classes))\n",
    "#         layers.append(nn.Softmax(dim=-1))\n",
    "\n",
    "#         self.base_model.fc = nn.Sequential(*layers)\n",
    "    \n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         return self.base_model(x)\n",
    "    \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            base_model, \n",
    "            dropout: float, \n",
    "            output_dims: List[int],\n",
    "            num_classes: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = base_model\n",
    "        if base_model.__class__.__name__ == 'ResNet':\n",
    "            input_dim: int = base_model.fc.in_features \n",
    "        elif base_model.__class__.__name__ == 'EfficientNet':\n",
    "            input_dim: int = base_model.classifier[1].in_features\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        for output_dim in output_dims:\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = output_dim\n",
    "        layers.append(nn.Linear(input_dim, num_classes))\n",
    "        layers.append(nn.Softmax(dim=-1))\n",
    "\n",
    "\n",
    "        if base_model.__class__.__name__ == 'ResNet':\n",
    "            self.base_model.fc = nn.Sequential(*layers) \n",
    "        elif base_model.__class__.__name__ == 'EfficientNet':\n",
    "            self.base_model.classifier = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.base_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = 'resnet50'\n",
    "base_model_name = 'efficientnet_b0'\n",
    "\n",
    "\n",
    "model = get_base_model(base_model_name)\n",
    "\n",
    "MODEL = Network(\n",
    "    base_model=get_base_model(base_model_name),\n",
    "    dropout=None,\n",
    "    output_dims=[],\n",
    "    num_classes=len(DERMNET_LABEL_NAME)\n",
    ") \n",
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12438, 3119, 4002)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data = prepare_dermnet_data()\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224]) torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, _, _ = local_dataloader(\n",
    "    resize_size=(256, 256, 3),\n",
    "    crop_size=(224, 224, 3),\n",
    "    batch_size=10,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "for image, label in train_dataloader:\n",
    "    print(image.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
